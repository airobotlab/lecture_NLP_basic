{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-sohfSFXnub"
      },
      "source": [
        "# SentencePiece VS Huggingface tokenizer\n",
        "\n",
        "한국어 서브워드 분절 알고리즘 실습&비교\n",
        "\n",
        "220527\n",
        "\n",
        "고우영"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6nqltMtXnuf"
      },
      "source": [
        "| 학습시간 비교(s) | 8000   | 16000 | 32000 | 64000 | 128000|\n",
        "|------|------|------|------|------|------|\n",
        "|   Sentencepiece  | 11| 22 |48| 110 |282|\n",
        "|   hugging_face  | 10| 11 |11| 12 |12|\n",
        "\n",
        "| 추론시간 비교(s) | 8000| 128000|\n",
        "|------|------|------|\n",
        "|   Sentencepiece  | 4.5| 4.93 |\n",
        "|   hugging_face  | 4.9| 4.97 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO9izbHlXnug"
      },
      "source": [
        "# NSMC 데이터셋 로드\n",
        "## 15만 문장, 113만 word(띄어쓰기 기준), 평균 7.5word/sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "ZCQPMJS2Xnug",
        "outputId": "c892c5eb-bc32-4977-de33-95211bc70a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('굳 ㅋ', 1), ('GDNTOPCLASSINTHECLUB', 0)]\n",
            "data loading done!\n",
            "sentence: ['굳 ㅋ', 'GDNTOPCLASSINTHECLUB', '뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아']\n",
            "Label   : [1, 0, 0]\n",
            "\n",
            "코퍼스 문장수/평균/총 단어 갯수 : 49997, 7.6 / 380472\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['굳 ㅋ', 'GDNTOPCLASSINTHECLUB', '뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아']\n",
            "\n",
            "코퍼스 문장수/평균/총 단어 갯수 : 49998, 7.6 / 380472\n",
            "CPU times: user 4.85 s, sys: 200 ms, total: 5.05 s\n",
            "Wall time: 6.18 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "## 1.2) read csv data\n",
        "# NSMC 데이터 로드\n",
        "import pandas as pd\n",
        "# data_nsmc = pd.read_csv('data/nsmc.txt', sep='\\t')\n",
        "data_nsmc = pd.read_table('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')\n",
        "\n",
        "train_pair = [(row[1], row[2]) for _, row in data_nsmc.iterrows() if type(row[1]) == str]  # nan 제거\n",
        "print(train_pair[:2])\n",
        "\n",
        "#  문장 및 라벨 데이터 추출\n",
        "train_data  = [pair[0] for pair in train_pair]\n",
        "train_label = [pair[1] for pair in train_pair]\n",
        "print('data loading done!')\n",
        "print('sentence: %s' %(train_data[:3]))\n",
        "print('Label   : %s' %(train_label[:3]))\n",
        "\n",
        "num_word_list = [len(sentence.split()) for sentence in train_data]\n",
        "print('\\n코퍼스 문장수/평균/총 단어 갯수 : %d, %.1f / %d' % (len(num_word_list), sum(num_word_list)/len(num_word_list), sum(num_word_list)))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(num_word_list, bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()\n",
        "\n",
        "# subword 학습을 위해 문장만 따로 저장\n",
        "with open('train_tokenizer.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in train_data:\n",
        "        f.write(line+'\\n')\n",
        "\n",
        "# subword 학습을 위해 문장만 따로 저장\n",
        "with open('train_tokenizer.txt', 'r', encoding='utf-8') as f:\n",
        "    test_tokenizer = f.read().split('\\n')\n",
        "print(test_tokenizer[:3])\n",
        "\n",
        "num_word_list = [len(sentence.split()) for sentence in test_tokenizer]\n",
        "print('\\n코퍼스 문장수/평균/총 단어 갯수 : %d, %.1f / %d' % (len(num_word_list), sum(num_word_list)/len(num_word_list), sum(num_word_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnePQPQmXnuh"
      },
      "source": [
        "# SentencePiece 학습"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "RdF34YDKXnui"
      },
      "source": [
        "# 설치\n",
        "!pip install sentencepiece==0.1.83"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece==0.1.83"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQzJK6yqYg2c",
        "outputId": "c7d044b1-65ae-41b8-fb57-32b8c19dc252"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece==0.1.83\n",
            "  Downloading sentencepiece-0.1.83-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 3.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWtdjWYiXnui",
        "outputId": "ed6bbdb4-8394-4999-8659-caf8c280c4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train done\n",
            "CPU times: user 20.6 s, sys: 252 ms, total: 20.9 s\n",
            "Wall time: 15.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# spm_train --input=train_tokenizer.txt  --model_prefix=sentencepiece/sp --vocab_size=32000 character_coverage=1.0 --model_type=\"unigram\"\n",
        "\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "\n",
        "input_file = 'train_tokenizer.txt'\n",
        "vocab_size = 32000\n",
        "sp_model_root='sentencepiece'\n",
        "if not os.path.isdir(sp_model_root):\n",
        "    os.mkdir(sp_model_root)\n",
        "sp_model_name = 'tokenizer_%d' % (vocab_size)\n",
        "sp_model_path = os.path.join(sp_model_root, sp_model_name)\n",
        "model_type = 'unigram'  # 학습할 모델 선택, unigram이 더 성능이 좋음'bpe'\n",
        "character_coverage  = 1.0  # 전체를 cover 하기 위해, default=0.9995\n",
        "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'\n",
        "\n",
        "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
        "cmd = input_argument%(input_file, sp_model_path, vocab_size,user_defined_symbols, model_type, character_coverage)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(cmd)\n",
        "print('train done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgfFlAAuXnuj",
        "outputId": "f00f1cd1-0189-4bc3-a082-3e703e4564e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[422, 668, 5753, 5305, 131, 121, 9622, 120]\n",
            "['▁나는', '▁오늘', '▁아침', '밥', '을', '▁', '먹었다', '.']\n",
            "나는 오늘 아침밥을 먹었다.\n",
            "나는 오늘 아침밥을 먹었다.\n"
          ]
        }
      ],
      "source": [
        "## check\n",
        "import sentencepiece as spm\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('{}.model'.format(sp_model_path))\n",
        "\n",
        "tokens = sp.encode_as_pieces('나는 오늘 아침밥을 먹었다.')\n",
        "ids = sp.encode_as_ids('나는 오늘 아침밥을 먹었다.')\n",
        "\n",
        "print(ids)\n",
        "print(tokens)\n",
        "\n",
        "tokens = sp.decode_pieces(tokens)\n",
        "ids = sp.decode_ids(ids)\n",
        "\n",
        "print(ids)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QDKjU5YXnuk"
      },
      "source": [
        "# Huggingface tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXhW3DfXnuk"
      },
      "source": [
        "## 1. Huggingface setup"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "cOv9aWZ3Xnuk"
      },
      "source": [
        "# hugging face tokenizer 설치\n",
        "!pip uninstall tokenizers\n",
        "!pip install tokenizers==0.9.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JwrwlLjXnuk"
      },
      "source": [
        "## 2. Huggingface train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers==0.12.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHHc_hHeZoDM",
        "outputId": "a1e0b870-5feb-424d-d609-66d2a77d9d4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tokenizers==0.12.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PS-7JsNXnul",
        "outputId": "2f8df3f4-a84c-4f8c-856d-a25699fc7425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertWordPieceTokenizer\n",
            "['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]']\n",
            "idx   : [4761, 4937, 6312, 3432, 2404, 1041, 4405, 45]\n",
            "tokens: ['나는', '오늘', '아침', '##밥', '##을', '먹', '##었다', '.']\n",
            "offset: [(0, 2), (3, 5), (6, 8), (8, 9), (9, 10), (11, 12), (12, 14), (14, 15)]\n",
            "idx   : [13943, 4497, 13513, 45, 45, 4645, 5424, 4471, 45, 45, 4388, 15204]\n",
            "tokens: ['교도소', '이야기', '##구먼', '.', '.', '솔직히', '재미는', '없다', '.', '.', '평점', '조정']\n",
            "offset: [(0, 3), (4, 7), (7, 9), (10, 11), (11, 12), (12, 15), (16, 19), (20, 22), (22, 23), (23, 24), (24, 26), (27, 29)]\n",
            "CPU times: user 5.81 s, sys: 1.33 s, total: 7.14 s\n",
            "Wall time: 6.41 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from tokenizers import SentencePieceBPETokenizer, BertWordPieceTokenizer\n",
        "\n",
        "how_to_tokenize = BertWordPieceTokenizer\n",
        "# how_to_tokenize = SentencePieceBPETokenizer\n",
        "\n",
        "# Initialize a tokenizer\n",
        "if str(how_to_tokenize) == str(BertWordPieceTokenizer):\n",
        "    print('BertWordPieceTokenizer')\n",
        "    tokenizer = how_to_tokenize(\n",
        "        strip_accents=False,  # Must be False if cased model\n",
        "        lowercase=False,\n",
        "    )\n",
        "elif str(how_to_tokenize) == str(SentencePieceBPETokenizer):\n",
        "    print('SentencePieceBPETokenizer')\n",
        "    tokenizer = how_to_tokenize()\n",
        "else:\n",
        "    assert('select right tokenizer')\n",
        "\n",
        "#########################################\n",
        "corpus_file   = ['train_tokenizer.txt']  # data path\n",
        "vocab_size    = 32000\n",
        "limit_alphabet= 6000\n",
        "output_path   = 'hugging_%d'%(vocab_size)\n",
        "\n",
        "hf_model_root='huggingface'\n",
        "if not os.path.isdir(hf_model_root):\n",
        "    os.mkdir(hf_model_root)\n",
        "hf_model_name = 'tokenizer_%d.json' % (vocab_size)\n",
        "hf_model_path = os.path.join(hf_model_root, hf_model_name)\n",
        "\n",
        "min_frequency = 5\n",
        "\n",
        "## 1) define special tokens\n",
        "user_defined_symbols = ['[BOS]','[EOS]','[UNK0]','[UNK1]','[UNK2]','[UNK3]','[UNK4]','[UNK5]','[UNK6]','[UNK7]','[UNK8]','[UNK9]']\n",
        "unused_token_num = 20\n",
        "unused_list = ['[unused{}]'.format(n) for n in range(unused_token_num)]\n",
        "special_tokens = user_defined_symbols + unused_list\n",
        "print(user_defined_symbols)\n",
        "\n",
        "\n",
        "# Then train it!\n",
        "tokenizer.train(files=corpus_file,\n",
        "               vocab_size=vocab_size,\n",
        "               min_frequency=min_frequency,  # 단어의 최소 발생 빈도, 5\n",
        "               limit_alphabet=limit_alphabet,\n",
        "               show_progress=True,\n",
        "               special_tokens=special_tokens\n",
        "               )\n",
        "\n",
        "# And finally save it somewhere\n",
        "tokenizer.save(hf_model_path)\n",
        "\n",
        "output = tokenizer.encode(\"나는 오늘 아침밥을 먹었다.\")\n",
        "print('idx   : %s'%output.ids)\n",
        "print('tokens: %s'%output.tokens)\n",
        "print('offset: %s'%output.offsets)\n",
        "\n",
        "output = tokenizer.encode(\"교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\")\n",
        "print('idx   : %s'%output.ids)\n",
        "print('tokens: %s'%output.tokens)\n",
        "print('offset: %s'%output.offsets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj8TtLjZXnul"
      },
      "source": [
        "## 3. Huggingface Tokenize test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm75iGYMXnul",
        "outputId": "b8b2d457-b566-47a0-f910-0bcbd7b4d8a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx   : [4761, 4937, 6312, 3432, 2404, 1041, 4405, 45]\n",
            "tokens: ['나는', '오늘', '아침', '##밥', '##을', '먹', '##었다', '.']\n",
            "offset: [(0, 2), (3, 5), (6, 8), (8, 9), (9, 10), (11, 12), (12, 14), (14, 15)]\n",
            "idx   : [13943, 4497, 13513, 45, 45, 4645, 5424, 4471, 45, 45, 4388, 15204]\n",
            "tokens: ['교도소', '이야기', '##구먼', '.', '.', '솔직히', '재미는', '없다', '.', '.', '평점', '조정']\n",
            "offset: [(0, 3), (4, 7), (7, 9), (10, 11), (11, 12), (12, 15), (16, 19), (20, 22), (22, 23), (23, 24), (24, 26), (27, 29)]\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "\n",
        "how_to_tokenize = BertWordPieceTokenizer\n",
        "# how_to_tokenize = SentencePieceBPETokenizer\n",
        "vocab_size    = 32000\n",
        "\n",
        "tokenizer = Tokenizer.from_file(hf_model_path)\n",
        "\n",
        "\n",
        "output = tokenizer.encode(\"나는 오늘 아침밥을 먹었다.\")\n",
        "print('idx   : %s'%output.ids)\n",
        "print('tokens: %s'%output.tokens)\n",
        "print('offset: %s'%output.offsets)\n",
        "\n",
        "output = tokenizer.encode(\"교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\")\n",
        "print('idx   : %s'%output.ids)\n",
        "print('tokens: %s'%output.tokens)\n",
        "print('offset: %s'%output.offsets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIDssrwMXnum"
      },
      "source": [
        "# Tokenze usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIOJIgPjXnum"
      },
      "source": [
        "## 1. SentencePiece Usage, load & 분절"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9x6w4srXnum",
        "outputId": "5519a56a-3e42-4b1d-bcd5-5a1b72f3d60f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁굳', '▁ᄏ']\n",
            "['▁G', 'D', 'N', 'T', 'O', 'P', 'CL', 'A', 'SS', 'IN', 'THE', 'CL', 'U', 'B']\n",
            "['▁뭐야', '▁이', '▁평점', '들은', '....', '▁나쁘진', '▁않지만', '▁10', '점', '▁짜리', '는', '▁더더욱', '▁아니잖아']\n",
            "CPU times: user 186 ms, sys: 1.89 ms, total: 187 ms\n",
            "Wall time: 254 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import sentencepiece as spm\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('{}.model'.format(sp_model_path))\n",
        "sentencepiece_tokenizer = sp.encode_as_pieces\n",
        "\n",
        "result_tokenized_sentencepiece = [sentencepiece_tokenizer(_tmp) for _tmp in test_tokenizer[:3]]\n",
        "    \n",
        "for tmp in result_tokenized_sentencepiece[:3]:\n",
        "    print(tmp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A2S1mojXnum"
      },
      "source": [
        "## 2. Huggingface Usage, load & 분절"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3NMVDyzXnum",
        "outputId": "31758123-e6a7-4f37-8bc2-cac42b05c398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['굳', 'ㅋ']\n",
            "['G', '##D', '##N', '##T', '##O', '##P', '##C', '##L', '##A', '##S', '##S', '##IN', '##T', '##H', '##EC', '##L', '##U', '##B']\n",
            "['뭐야', '이', '평점들', '##은', '.', '.', '.', '.', '나쁘진', '않지만', '10점', '짜리', '##는', '더더욱', '아니잖아']\n",
            "CPU times: user 29.7 ms, sys: 1.98 ms, total: 31.7 ms\n",
            "Wall time: 33.4 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "how_to_tokenize = BertWordPieceTokenizer\n",
        "# how_to_tokenize = SentencePieceBPETokenizer\n",
        "vocab_size    = 32000\n",
        "\n",
        "tokenizer = Tokenizer.from_file(hf_model_path)\n",
        "\n",
        "result_tokenized_sentencepiece = [tokenizer.encode(_tmp).tokens for _tmp in test_tokenizer[:3]]\n",
        "for tmp in result_tokenized_sentencepiece[:3]:\n",
        "    print(tmp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bEJoj-3kXnun"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FUwzg1TgXnun"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OyGOgendXnun"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "nlp_7_2_subword_sentencepiece_VS_huggingface_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}