{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvcKzJ-zKdzk"
      },
      "source": [
        "# Text classification with CNN advanced\n",
        "\n",
        "220603\n",
        "\n",
        "- [ref](https://towardsdatascience.com/text-classification-with-cnns-in-pytorch-1113df31e79f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## download data\n",
        "!git clone https://github.com/airobotlab/lecture_NLP_basic\n",
        "!mv lecture_NLP_basic/data .\n",
        "!rm -rf lecture_NLP_basic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGhfaBOKLf_Q",
        "outputId": "e0458f7f-baf2-46c7-e7e1-52219d57b981"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lecture_NLP_basic'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 52 (delta 0), reused 4 (delta 0), pack-reused 48\u001b[K\n",
            "Unpacking objects: 100% (52/52), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "code_folding": [
          0
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFGcQpx7KoCV",
        "outputId": "f895da11-5f2b-4bb7-8061-f10308f79296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "## import\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "## fix seed\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "np.random.seed(0)\n",
        "cudnn.benchmark = False\n",
        "cudnn.deterministic = True\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "code_folding": [
          0,
          9,
          22
        ],
        "id": "7VNKkCmNKoCW"
      },
      "outputs": [],
      "source": [
        "# download dataset, https://www.kaggle.com/competitions/nlp-getting-started/data\n",
        "\n",
        "model_type = 'CNN'\n",
        "# model_type = 'RNN'\n",
        "\n",
        "\n",
        "data_train = 'data/nlp_disaster_tweets_train.csv'\n",
        "\n",
        "class CNN_config_define:\n",
        "    def __init__(self):\n",
        "        self.num_words= 2000\n",
        "        self.seq_len = 35\n",
        "        \n",
        "        self.embedding_size= 100\n",
        "        self.out_size= 32\n",
        "        self.stride= 2\n",
        "        \n",
        "        self.batch_size = 12\n",
        "        self.epochs = 20\n",
        "        self.learning_rate= 0.001\n",
        "\n",
        "class LSTM_config_define:\n",
        "    def __init__(self):\n",
        "        self.num_words= 2000\n",
        "        self.seq_len = 35\n",
        "        \n",
        "        self.embedding_size= 100\n",
        "        self.hidden_dim= 32  \n",
        "        self.lstm_layers= 3\n",
        "        \n",
        "        self.batch_size = 12\n",
        "        self.epochs = 20\n",
        "        self.learning_rate= 0.001\n",
        "        \n",
        "if model_type == 'CNN':\n",
        "    config = CNN_config_define()\n",
        "elif model_type == 'RNN':\n",
        "    config = LSTM_config_define()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "code_folding": [
          0,
          16,
          34,
          52
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2H7ZmplKoCX",
        "outputId": "544cd466-4d0e-418d-ba1b-e19eea190da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forest fire near la ronge sask canada \n",
            " -> ['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada']\n",
            " -> [208, 49, 240, 710, 1198]\n",
            " -> [ 208   49  240  710 1198    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0]\n",
            "train: (6090, 35) / (6090,)  - valid: (1523, 35) / (1523,)\n"
          ]
        }
      ],
      "source": [
        "## prepare dataset\n",
        "df = pd.read_csv(data_train)\n",
        "df.drop(['id', 'keyword', 'location'], axis=1, inplace=True)\n",
        "x_raw = df['text'].values\n",
        "y = df['target'].values\n",
        "\n",
        "# 1) clean_text\n",
        "x_raw_clean = [x.lower() for x in x_raw]\n",
        "x_raw_clean = [re.sub(r'[^A-Za-z]+', ' ', x) for x in x_raw_clean]\n",
        "\n",
        "# 2) text_tokenization\n",
        "x_raw_tokenization = [word_tokenize(x) for x in x_raw_clean]\n",
        "\n",
        "print('%s \\n -> %s'%(x_raw_clean[1], x_raw_tokenization[1]))\n",
        "\n",
        "# 3) build_vocabulary\n",
        "def build_vocabulary(x_raw_tokenization, num_words=2000):\n",
        "    # Builds the vocabulary and keeps the \"x\" most frequent word\n",
        "    vocabulary = dict()\n",
        "    fdist = nltk.FreqDist()\n",
        "\n",
        "    for sentence in x_raw_tokenization:\n",
        "        for word in sentence:\n",
        "            fdist[word] += 1\n",
        "\n",
        "    common_words = fdist.most_common(num_words)\n",
        "\n",
        "    for idx, word in enumerate(common_words):\n",
        "        vocabulary[word[0]] = (idx+1)\n",
        "    return vocabulary\n",
        "\n",
        "vocabulary = build_vocabulary(x_raw_tokenization, num_words=2000)\n",
        "\n",
        "# 4) word_to_idx\n",
        "def word_to_idx(vocabulary, x_raw_tokenization):\n",
        "    # By using the dictionary (vocabulary), it is transformed\n",
        "    # each token into its index based representatio\n",
        "    x_tokenized = list()\n",
        "\n",
        "    for sentence in x_raw_tokenization:\n",
        "        temp_sentence = list()\n",
        "        for word in sentence:\n",
        "            if word in vocabulary.keys():\n",
        "                temp_sentence.append(vocabulary[word])\n",
        "        x_tokenized.append(temp_sentence)\n",
        "        \n",
        "    return x_tokenized\n",
        "\n",
        "x_tokenized = word_to_idx(vocabulary, x_raw_tokenization)\n",
        "print(' -> %s'%(x_tokenized[1]))\n",
        "\n",
        "# 5) padding_sentences\n",
        "def padding_sentences(x_tokenized, seq_len=35):\n",
        "    # Each sentence which does not fulfill the required le\n",
        "    # it's padded with the index 0\n",
        "    pad_idx = 0\n",
        "    x_padded = list()\n",
        "\n",
        "    for sentence in x_tokenized:\n",
        "        while len(sentence) < seq_len:\n",
        "            sentence.insert(len(sentence), pad_idx)\n",
        "        x_padded.append(sentence)\n",
        "\n",
        "    x_padded = np.array(x_padded)\n",
        "    return x_padded\n",
        "    \n",
        "x_padded = padding_sentences(x_tokenized, seq_len=config.seq_len)\n",
        "print(' -> %s'%(x_padded[1]))\n",
        "\n",
        "# 6) split data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_padded, y, test_size=0.2, random_state=42)\n",
        "print('train: %s / %s  - valid: %s / %s' %(x_train.shape, y_train.shape, x_test.shape, y_test.shape))\n",
        "\n",
        "data = {}\n",
        "data['x_train'] = x_train\n",
        "data['y_train'] = y_train\n",
        "data['x_test']  = x_test\n",
        "data['y_test']  = y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "3E-dunmgKoCY"
      },
      "outputs": [],
      "source": [
        "# make dataloader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class DatasetMapper(Dataset):\n",
        "\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "    \n",
        "\n",
        "# Initialize dataset maper\n",
        "dataset_train = DatasetMapper(data['x_train'], data['y_train'])\n",
        "dataset_test = DatasetMapper(data['x_test'], data['y_test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "code_folding": [
          0,
          1,
          5,
          17
        ],
        "id": "RL_uCJ7cKoCZ"
      },
      "outputs": [],
      "source": [
        "# functions\n",
        "def count_parameters(model):\n",
        "    print('model parameters: %d'%(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
        "    return\n",
        "\n",
        "def evaluation(model, loader_test):\n",
        "    # Set the model in evaluation mode\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    # Starst evaluation phase\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in loader_test:\n",
        "            y_pred = model(x_batch)\n",
        "            predictions += list(y_pred.detach().numpy())\n",
        "    return predictions\n",
        "\n",
        "def calculate_accuray(grand_truth, predictions):\n",
        "    true_positives = 0\n",
        "    true_negatives = 0\n",
        "\n",
        "    # Gets frequency  of true positives and true negatives\n",
        "    # The threshold is 0.5\n",
        "    for true, pred in zip(grand_truth, predictions):\n",
        "        if (pred >= 0.5) and (true == 1):\n",
        "            true_positives += 1\n",
        "        elif (pred < 0.5) and (true == 0):\n",
        "            true_negatives += 1\n",
        "        else:\n",
        "            pass\n",
        "    # Return accuracy\n",
        "    return (true_positives+true_negatives) / len(grand_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "code_folding": [
          0,
          6,
          53,
          97
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMSMUsiCKoCZ",
        "outputId": "9b81a9ac-d27b-4b59-e68e-d695dffca210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model parameters: 218917\n"
          ]
        }
      ],
      "source": [
        "# define CNN model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class CNN1D_Classifier(nn.ModuleList):\n",
        "    def __init__(self, config):\n",
        "        super(CNN1D_Classifier, self).__init__()\n",
        "\n",
        "        # Parameters regarding text preprocessing\n",
        "        self.seq_len = config.seq_len\n",
        "        self.num_words = config.num_words\n",
        "        self.embedding_size = config.embedding_size\n",
        "\n",
        "        # Dropout definition\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "        # CNN parameters definition\n",
        "        # Kernel sizes\n",
        "        self.kernel_1 = 2\n",
        "        self.kernel_2 = 3\n",
        "        self.kernel_3 = 4\n",
        "        self.kernel_4 = 5\n",
        "\n",
        "        # Output size for each convolution\n",
        "        self.out_size = config.out_size\n",
        "        # Number of strides for each convolution\n",
        "        self.stride = config.stride\n",
        "\n",
        "        # Embedding layer definition\n",
        "        self.embedding = nn.Embedding(\n",
        "            self.num_words + 1, self.embedding_size, padding_idx=0)\n",
        "\n",
        "        # Convolution layers definition\n",
        "        self.conv_1 = nn.Conv1d(\n",
        "            self.seq_len, self.out_size, self.kernel_1, self.stride)\n",
        "        self.conv_2 = nn.Conv1d(\n",
        "            self.seq_len, self.out_size, self.kernel_2, self.stride)\n",
        "        self.conv_3 = nn.Conv1d(\n",
        "            self.seq_len, self.out_size, self.kernel_3, self.stride)\n",
        "        self.conv_4 = nn.Conv1d(\n",
        "            self.seq_len, self.out_size, self.kernel_4, self.stride)\n",
        "\n",
        "        # Max pooling layers definition\n",
        "        self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
        "        self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
        "        self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
        "        self.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n",
        "\n",
        "        # Fully connected layer definition\n",
        "        self.fc = nn.Linear(self.in_features_fc(), 1)\n",
        "\n",
        "        \n",
        "    def in_features_fc(self):\n",
        "        '''Calculates the number of output features after Convolution + Max pooling\n",
        "\n",
        "        Convolved_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
        "        Pooled_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
        "\n",
        "        source: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
        "        '''\n",
        "        # Calcualte size of convolved/pooled features for convolution_1/max_pooling_1 features\n",
        "        out_conv_1 = ((self.embedding_size - 1 *\n",
        "                      (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
        "        out_conv_1 = math.floor(out_conv_1)\n",
        "        out_pool_1 = (\n",
        "            (out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
        "        out_pool_1 = math.floor(out_pool_1)\n",
        "\n",
        "        # Calcualte size of convolved/pooled features for convolution_2/max_pooling_2 features\n",
        "        out_conv_2 = ((self.embedding_size - 1 *\n",
        "                      (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
        "        out_conv_2 = math.floor(out_conv_2)\n",
        "        out_pool_2 = (\n",
        "            (out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
        "        out_pool_2 = math.floor(out_pool_2)\n",
        "\n",
        "        # Calcualte size of convolved/pooled features for convolution_3/max_pooling_3 features\n",
        "        out_conv_3 = ((self.embedding_size - 1 *\n",
        "                      (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
        "        out_conv_3 = math.floor(out_conv_3)\n",
        "        out_pool_3 = (\n",
        "            (out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
        "        out_pool_3 = math.floor(out_pool_3)\n",
        "\n",
        "        # Calcualte size of convolved/pooled features for convolution_4/max_pooling_4 features\n",
        "        out_conv_4 = ((self.embedding_size - 1 *\n",
        "                      (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
        "        out_conv_4 = math.floor(out_conv_4)\n",
        "        out_pool_4 = (\n",
        "            (out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
        "        out_pool_4 = math.floor(out_pool_4)\n",
        "\n",
        "        # Returns \"flattened\" vector (input for fully connected layer)\n",
        "        return (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4) * self.out_size\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Sequence of tokes is filterd through an embedding layer\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Convolution layer 1 is applied\n",
        "        x1 = self.conv_1(x)\n",
        "        x1 = torch.relu(x1)\n",
        "        x1 = self.pool_1(x1)\n",
        "\n",
        "        # Convolution layer 2 is applied\n",
        "        x2 = self.conv_2(x)\n",
        "        x2 = torch.relu((x2))\n",
        "        x2 = self.pool_2(x2)\n",
        "\n",
        "        # Convolution layer 3 is applied\n",
        "        x3 = self.conv_3(x)\n",
        "        x3 = torch.relu(x3)\n",
        "        x3 = self.pool_3(x3)\n",
        "\n",
        "        # Convolution layer 4 is applied\n",
        "        x4 = self.conv_4(x)\n",
        "        x4 = torch.relu(x4)\n",
        "        x4 = self.pool_4(x4)\n",
        "\n",
        "        # The output of each convolutional layer is concatenated into a unique vector\n",
        "        union = torch.cat((x1, x2, x3, x4), 2)\n",
        "        union = union.reshape(union.size(0), -1)\n",
        "\n",
        "        # The \"flattened\" vector is passed through a fully connected layer\n",
        "        out = self.fc(union)\n",
        "        # Dropout is applied\n",
        "        out = self.dropout(out)\n",
        "        # Activation function is applied\n",
        "        out = torch.sigmoid(out)\n",
        "\n",
        "        return out.squeeze()\n",
        "    \n",
        "model = CNN1D_Classifier(config)\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "code_folding": [
          0
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIkM3NFaKoCa",
        "outputId": "2e5ec870-c7ab-461b-c1a2-58d21eaff92e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, loss: 0.73590, Train accuracy: 0.56749, Test accuracy: 0.69140\n",
            "Epoch: 2, loss: 0.75063, Train accuracy: 0.67011, Test accuracy: 0.71766\n",
            "Epoch: 3, loss: 0.40161, Train accuracy: 0.71117, Test accuracy: 0.72489\n",
            "Epoch: 4, loss: 0.19537, Train accuracy: 0.75484, Test accuracy: 0.73736\n",
            "Epoch: 5, loss: 0.51257, Train accuracy: 0.78407, Test accuracy: 0.70519\n",
            "Epoch: 6, loss: 0.26615, Train accuracy: 0.80148, Test accuracy: 0.74130\n",
            "Epoch: 7, loss: 0.11793, Train accuracy: 0.81938, Test accuracy: 0.72423\n",
            "Epoch: 8, loss: 0.02671, Train accuracy: 0.82594, Test accuracy: 0.69928\n",
            "Epoch: 9, loss: 0.11771, Train accuracy: 0.83218, Test accuracy: 0.70322\n",
            "Epoch: 10, loss: 0.11669, Train accuracy: 0.82775, Test accuracy: 0.70716\n",
            "Epoch: 11, loss: 0.23247, Train accuracy: 0.83383, Test accuracy: 0.70519\n",
            "Epoch: 12, loss: 0.12048, Train accuracy: 0.84039, Test accuracy: 0.69993\n",
            "Epoch: 13, loss: 0.23695, Train accuracy: 0.84302, Test accuracy: 0.74852\n",
            "Epoch: 14, loss: 0.00423, Train accuracy: 0.85090, Test accuracy: 0.74196\n",
            "Epoch: 15, loss: 0.23118, Train accuracy: 0.84023, Test accuracy: 0.70190\n",
            "Epoch: 16, loss: 0.34719, Train accuracy: 0.84516, Test accuracy: 0.69796\n",
            "Epoch: 17, loss: 0.34696, Train accuracy: 0.84943, Test accuracy: 0.70125\n",
            "Epoch: 18, loss: 0.23165, Train accuracy: 0.83842, Test accuracy: 0.72357\n",
            "Epoch: 19, loss: 0.11590, Train accuracy: 0.84039, Test accuracy: 0.71110\n",
            "Epoch: 20, loss: 0.34727, Train accuracy: 0.84532, Test accuracy: 0.73736\n",
            "train done!!, 65.2 sec\n"
          ]
        }
      ],
      "source": [
        "## train!!\n",
        "# Initialize loaders\n",
        "loader_train = DataLoader(dataset_train, batch_size=config.batch_size)\n",
        "loader_test = DataLoader(dataset_test, batch_size=config.batch_size)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "# Starts training phase\n",
        "start_time = time.time()\n",
        "for epoch in range(config.epochs):\n",
        "    # Set model in training model\n",
        "    model.train()\n",
        "    predictions = []\n",
        "    # Starts batch training\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(loader_train):\n",
        "\n",
        "        y_batch = y_batch.type(torch.FloatTensor)\n",
        "\n",
        "        # Feed the model\n",
        "        y_pred = model(x_batch)\n",
        "\n",
        "        # Loss calculation\n",
        "        loss = F.binary_cross_entropy(y_pred, y_batch)\n",
        "\n",
        "        # Clean gradientes\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Gradients calculation\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradients update\n",
        "        optimizer.step()\n",
        "\n",
        "        # Save predictions\n",
        "        predictions += list(y_pred.detach().numpy())\n",
        "\n",
        "    # Evaluation phase\n",
        "    test_predictions = evaluation(model, loader_test)\n",
        "\n",
        "    # Metrics calculation\n",
        "    train_accuary = calculate_accuray(data['y_train'], predictions)\n",
        "    test_accuracy = calculate_accuray(data['y_test'], test_predictions)\n",
        "    print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f, Test accuracy: %.5f\" % (\n",
        "        epoch+1, loss.item(), train_accuary, test_accuracy))\n",
        "    \n",
        "print('train done!!, %.1f sec'%(time.time()-start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P339cm8vKoCa"
      },
      "source": [
        "# 2) Text classification with LSTM advanced\n",
        "\n",
        "220603\n",
        "\n",
        "- [ref](https://towardsdatascience.com/text-classification-with-pytorch-7111dae111a6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "r_FW72TLKoCb"
      },
      "outputs": [],
      "source": [
        "# model_type = 'CNN'\n",
        "model_type = 'RNN'\n",
        "\n",
        "\n",
        "data_train = 'data/nlp_disaster_tweets/train.csv'\n",
        "data_test = 'data/nlp_disaster_tweets/test.csv'\n",
        "\n",
        "class CNN_config_define:\n",
        "    def __init__(self):\n",
        "        self.num_words= 2000\n",
        "        self.seq_len = 35\n",
        "        \n",
        "        self.embedding_size= 100\n",
        "        self.out_size= 32\n",
        "        self.stride= 2\n",
        "        \n",
        "        self.batch_size = 12\n",
        "        self.epochs = 20\n",
        "        self.learning_rate= 0.001\n",
        "\n",
        "class LSTM_config_define:\n",
        "    def __init__(self):\n",
        "        self.num_words= 2000\n",
        "        self.seq_len = 35\n",
        "        \n",
        "        self.embedding_size= 100\n",
        "        self.hidden_dim= 32  \n",
        "        self.lstm_layers= 3\n",
        "        \n",
        "        self.batch_size = 12\n",
        "        self.epochs = 20\n",
        "        self.learning_rate= 0.001\n",
        "        \n",
        "if model_type == 'CNN':\n",
        "    config = CNN_config_define()\n",
        "elif model_type == 'RNN':\n",
        "    config = LSTM_config_define()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "code_folding": [
          0
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ1DJ-bwKoCb",
        "outputId": "4efe61ce-a456-4c45-b869-bd4290ab8e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model parameters: 91553\n"
          ]
        }
      ],
      "source": [
        "# define LSTM model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class LSTM_Classifier(nn.ModuleList):\n",
        "    def __init__(self, config):\n",
        "        super(LSTM_Classifier, self).__init__()\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.batch_size = config.batch_size\n",
        "        self.hidden_dim = config.hidden_dim\n",
        "        self.LSTM_layers = config.lstm_layers\n",
        "        self.input_size = config.num_words\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        # Embedding layer definition\n",
        "        self.embedding = nn.Embedding(\n",
        "            self.input_size+ 1, self.hidden_dim, padding_idx=0)\n",
        "        \n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim,\n",
        "                            num_layers=self.LSTM_layers, batch_first=True)\n",
        "        # FC\n",
        "        self.fc1 = nn.Linear(in_features=self.hidden_dim,\n",
        "                             out_features=self.hidden_dim*2)\n",
        "        self.fc2 = nn.Linear(self.hidden_dim*2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Hidden and cell state definion\n",
        "        h = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n",
        "        c = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n",
        "\n",
        "        # Initialization fo hidden and cell states\n",
        "        torch.nn.init.xavier_normal_(h)\n",
        "        torch.nn.init.xavier_normal_(c)\n",
        "\n",
        "        # Each sequence \"x\" is passed through an embedding layer\n",
        "        out = self.embedding(x)\n",
        "        # Feed LSTMs\n",
        "        out, (hidden, cell) = self.lstm(out, (h, c))\n",
        "        out = self.dropout(out)\n",
        "        # The last hidden state is taken\n",
        "        out = torch.relu_(self.fc1(out[:, -1, :]))\n",
        "        out = self.dropout(out)\n",
        "        out = torch.sigmoid(self.fc2(out))\n",
        "\n",
        "        return out.squeeze()\n",
        "    \n",
        "model = LSTM_Classifier(config)\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "code_folding": [
          0
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBXAna5AKoCc",
        "outputId": "f6677a26-0395-4568-aeef-83a3417118b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, loss: 0.54597, Train accuracy: 0.57455, Test accuracy: 0.62114\n",
            "Epoch: 2, loss: 0.58746, Train accuracy: 0.61921, Test accuracy: 0.63165\n",
            "Epoch: 3, loss: 0.51694, Train accuracy: 0.63695, Test accuracy: 0.63887\n",
            "Epoch: 4, loss: 0.74323, Train accuracy: 0.66962, Test accuracy: 0.68746\n",
            "Epoch: 5, loss: 0.74258, Train accuracy: 0.75320, Test accuracy: 0.72685\n",
            "Epoch: 6, loss: 0.76657, Train accuracy: 0.78358, Test accuracy: 0.74130\n",
            "Epoch: 7, loss: 0.68691, Train accuracy: 0.80985, Test accuracy: 0.76362\n",
            "Epoch: 8, loss: 0.50656, Train accuracy: 0.82808, Test accuracy: 0.77019\n",
            "Epoch: 9, loss: 0.43926, Train accuracy: 0.84154, Test accuracy: 0.77282\n",
            "Epoch: 10, loss: 0.49607, Train accuracy: 0.85731, Test accuracy: 0.78267\n",
            "Epoch: 11, loss: 0.22675, Train accuracy: 0.86765, Test accuracy: 0.78529\n",
            "Epoch: 12, loss: 0.22957, Train accuracy: 0.88259, Test accuracy: 0.79383\n",
            "Epoch: 13, loss: 0.24916, Train accuracy: 0.89146, Test accuracy: 0.78661\n",
            "Epoch: 14, loss: 0.11880, Train accuracy: 0.89589, Test accuracy: 0.77544\n",
            "Epoch: 15, loss: 0.08805, Train accuracy: 0.90673, Test accuracy: 0.78004\n",
            "Epoch: 16, loss: 0.21915, Train accuracy: 0.90920, Test accuracy: 0.77610\n",
            "Epoch: 17, loss: 0.10433, Train accuracy: 0.91576, Test accuracy: 0.78398\n",
            "Epoch: 18, loss: 0.05531, Train accuracy: 0.92020, Test accuracy: 0.77676\n",
            "Epoch: 19, loss: 0.06902, Train accuracy: 0.92463, Test accuracy: 0.77610\n",
            "Epoch: 20, loss: 0.09650, Train accuracy: 0.93038, Test accuracy: 0.78398\n",
            "train done!!, 167.3 sec\n"
          ]
        }
      ],
      "source": [
        "## train!!\n",
        "# Initialize loaders\n",
        "loader_train = DataLoader(dataset_train, batch_size=config.batch_size)\n",
        "loader_test = DataLoader(dataset_test, batch_size=config.batch_size)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "# Starts training phase\n",
        "start_time = time.time()\n",
        "for epoch in range(config.epochs):\n",
        "    # Set model in training model\n",
        "    model.train()\n",
        "    predictions = []\n",
        "    # Starts batch training\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(loader_train):\n",
        "\n",
        "        y_batch = y_batch.type(torch.FloatTensor)\n",
        "\n",
        "        # Feed the model\n",
        "        y_pred = model(x_batch)\n",
        "\n",
        "        # Loss calculation\n",
        "        loss = F.binary_cross_entropy(y_pred, y_batch)\n",
        "\n",
        "        # Clean gradientes\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Gradients calculation\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradients update\n",
        "        optimizer.step()\n",
        "\n",
        "        # Save predictions\n",
        "        predictions += list(y_pred.detach().numpy())\n",
        "\n",
        "    # Evaluation phase\n",
        "    test_predictions = evaluation(model, loader_test)\n",
        "\n",
        "    # Metrics calculation\n",
        "    train_accuary = calculate_accuray(data['y_train'], predictions)\n",
        "    test_accuracy = calculate_accuray(data['y_test'], test_predictions)\n",
        "    print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f, Test accuracy: %.5f\" % (\n",
        "        epoch+1, loss.item(), train_accuary, test_accuracy))\n",
        "    \n",
        "print('train done!!, %.1f sec'%(time.time()-start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iGf10xMPKoCc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nMC_BJ3RKdzs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_4_text_cnn_고급_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}